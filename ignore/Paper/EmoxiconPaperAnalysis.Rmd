---
title: "Emoxicon Paper Analysis"
author: "Tara L. Valladares"
date: "4/30/2020"
output: html_document
# output: word_document
# output: html_notebook
---

```{r setup, include=FALSE}
# library(knitr)
# purl("ignore/Paper/EmoxiconPaperAnalysis.Rmd", output = "EmoxiconPaperAnalysis-April-30-2020.R", documentation = 0) # Extract R code from Rmd document and discard all text chunks
library(papaja)
library(emoxicon)
library(eRm)
library(ggplot2)
library(boot)
library(infotheo)
library(psych)
library(mirt)
```

## Prepare Trolls Data 

This data was saved into the emoxicon package. It was not run here for speed. Cleaning included:

* Only english-language tweets
* Only right/left trolls
* Remove duplicate tweets
* Remove manual reposts (i.e., those labeled with "rt")
* Remove links 
* Remove empty tweets (e.g., used to have a link, was just an image, etc.)

```{r}
## code to prepare `trolls` dataset

### not run ###
# trolls_raw <- vector("list", ndoc)
# for(i in 1:ndoc){
#   trolls_raw[[i]]<- read.csv(paste0("../russian-troll-tweets-master/IRAhandle_tweets_", i,".csv"),
#               stringsAsFactors = FALSE)
# }
# trolls<-vector("list",ndoc)
# for(i in 1:ndoc){
#   trolls[[i]]<- trolls_raw[[i]][which(trolls_raw[[i]]$language == "English"),]
# }
# 
# # Remove retweets
# trolls.clean <- lapply(trolls,function(x){
#   x<-x[which(x$retweet == 0),]
# } )
# 
# # clean data
# trolls.clean <- lapply(trolls.clean, function(y){
#   content<- y$content
#   content<- stringi::stri_trans_general(content, "latin-ascii")
#   content<- gsub("&amp", "and", content)
#   content<- gsub("http[^[:space:]]*", "", content) # remove links
#   content<- tolower(content)
#   content<- gsub("^rt.*", "", content) # remove reposts
# 
#   results<- cbind(content, y[,c("author", "publish_date", "followers", "updates","account_type")])
#   results<- results[which(results$account_type=="Left"|
#                             results$account_type=="Right"),]
#   results<- subset(results, content !="") #remove empty tweets
# })
# 
# trolls<-do.call(rbind, trolls.clean)
# trolls$content<- as.character(trolls$content)

```

## Run Emoxicon

Now we run the emoxicon scoring. I excluded the following terms:

```{r, echo=FALSE}
exclude <-c("hilary", "hillary", "russia", "russian",
            "trump","donald", "bernie", "sanders","clinton", "rt",
            "obama", "barack", "america", "president",
            "black", "white", "racist")

(exclude %in% emotions$word)
```
*`r exclude`*

These terms were excluded as they are hypothesized not to have the same emotional valence between groups. If I kept them in, they would inflate those emotional categories.

```{r models}
# Load data from Emoxicon package
data(trolls)

# Create vector of unique author names w/ more than 30 tweets, for later
author.small <- unique(trolls[c("author", "account_type")])
author.small <-  author.small[order(author.small$author),] # this is necessary b/c table() puts it into alphabetical order
author.small<- author.small[as.vector(table(trolls$author) > 29),]

# Run Emoxicon

trolls_scored <- emoxicon(text=trolls$content, lexicon = "emotions",
                          exclude = exclude)

# Run Rasch models

trolls_models <- rasch(scores= trolls_scored, groups = trolls$author, return_models = TRUE)
```

The author.small vector includes the correct authors - the same ones in the emoxicon output. BUT the ordering is not the same due to some alphabetical quirks. This is why the results below with use the `%in%` function to subset data.

```{r}
# Confirm that author.small matches the individual trolls_models
x <- sapply(trolls_models$group_models, function(z){z$group})

# If all are true, then both match contents
length(x) == length(author.small$author)
sum(x %in% author.small$author) == length(x)
sum(author.small$author %in% x) == length(author.small$author)

# If true, then the order is also the same
z<-rep(NA, length(trolls_models$group_model))
for(i in 1:length(trolls_models$group_model)){
  z[i] <- trolls_models$group_models[[i]]$group == author.small$author[i]
}
all(z)

```

## Fit

Now we can look at fit parameters. 

```{r}
# Fit
summary(trolls_models$full_model)

pparameters <- eRm::person.parameter(trolls_models$full_model)

pfit <- eRm::personfit(pparameters)
ifit <- eRm::itemfit(pparameters)
pmis <- eRm::PersonMisfit(pparameters)

ifit
pmis

```

Low levels of misfit across persons. It does not seem to be influenced by raw score.

```{r}
# infit/outfit plots by theta-----
plotdat <- data.frame(theta = as.factor(pparameters$thetapar[[1]]),
                      infitZ =pfit$p.infitZ,
                      outfitZ =pfit$p.outfitZ,
                      se = pparameters$se.theta$NAgroup1)
levels(plotdat$theta) <- round(as.numeric(levels(plotdat$theta)),2)

ggplot(data = plotdat, aes(x = theta, y=infitZ)) +
  # geom_jitter(aes(x = theta, y=infitZ),height = .1, width = .1)
  geom_violin() + geom_hline(yintercept = c(2,-2), col="#D55E00") +
  ylim(-3,3) + ggtitle("Standardized Infit")

ggplot(data = plotdat, aes(x = theta, y=outfitZ)) +
  # geom_jitter(aes(x = theta, y=infitZ),height = .1, width = .1)
  geom_violin() + geom_hline(yintercept = c(2,-2), col="#D55E00") +
  ylim(-3,3) + ggtitle("Standardized Outfit")

ggplot(data = unique(plotdat), aes(x = theta, y=se)) +
  # geom_jitter(aes(x = theta, y=infitZ),height = .1, width = .1)
  geom_point() + ggtitle("Standard Error of the Estimate")
```

## DIF, overall model

Doesn't seem to be a large amount of DIF between low and high scores.

There seems to be DIF between left and right trolls. Specifically, INSPIRRED is harder for Right trolls. ANNOYED, AMUSED, AFRAID are harder for Left trolls.

```{r dif, fig.height=4}
# fit plots -----
# person item map
plotPImap(trolls_models$full_model, sorted = TRUE)

# check if high and low scores are the same
lrres.rasch <- LRtest(trolls_models$full_model, splitcr = "median")
plotGOF(lrres.rasch, tlab = "item",
        conf = list(ia = FALSE, col = "blue", lty = "dotted"),
        smooline= list(gamma = .95, col = "black", lty = "dashed"))

# check if left and right trolls are the same
lrres <- LRtest(trolls_models$full_model, splitcr = trolls$account_type)
plotGOF(lrres, tlab = "item",
        conf = list(ia = FALSE, col = "red", lty = "dotted"),
        smooline = list(gamma = .95, col = "black", lty = "dashed"))
```


## ICCs

ICCs look great. Always monotonically increasing.

```{r icc}
# Look at empiracal plots, ICCs

# plotICC(trolls_models$full_model,1, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,2, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,3, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,4, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,5, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,6, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,7, empICC = list("raw", type="b",col = "blue", lty = 2))
# plotICC(trolls_models$full_model,8, empICC = list("raw", type="b",col = "blue", lty = 2))

```

## Empirical plots

Here we are plotting reduced total raw scores against the proportion of people who endorsed the item. These are very flat and pretty similar. This matches the item difficulties - they for the most part all quite similar. Worrisome? Not sure. 

```{r empirical plots}

# Look at empirical plots - total scores
# library(mirt)
# empirical_plot(trolls_models$full_model$X
#                ,1)
# empirical_plot(trolls_models$full_model$X
#                ,2)
# empirical_plot(trolls_models$full_model$X
#                ,3)
# empirical_plot(trolls_models$full_model$X
#                ,4)
# empirical_plot(trolls_models$full_model$X
#                ,5)
# empirical_plot(trolls_models$full_model$X
#                ,6)
# empirical_plot(trolls_models$full_model$X
#                ,7)
# empirical_plot(trolls_models$full_model$X
#                ,8)
```

## Individual model fit statistics

```{r indv fit}
# create vector to subset left models that is in the correct order
modsL <- sapply(trolls_models$group_models, function(x) x$group) # extract groups
modsL <- modsL %in% author.small$author[which(author.small$account_type == "Left")] # match

# Run fit functions on all models individually
manymods_pparameters <- lapply(trolls_models$group_models, function(x){
  eRm::person.parameter(x$model)
})
manymods_pfit <- lapply(1:length(trolls_models$group_models), function(x){
  eRm::personfit(manymods_pparameters[[x]])
})
manymods_ifit <- lapply(1:length(trolls_models$group_models), function(x){
  eRm::itemfit(manymods_pparameters[[x]])
})
manymods_pmis <- lapply(1:length(trolls_models$group_models), function(x){
  eRm::PersonMisfit(manymods_pparameters[[x]])
})
manymods_pmis_total <- sapply(manymods_pmis, function(x){x$PersonMisfit})

summary(manymods_pmis_total)
summary(manymods_pmis_total[modsL])
summary(manymods_pmis_total[!modsL])

# look at misfitting items within individual models
manymods_ifit_total <- sapply(1:length(manymods_ifit), function(i){
  p <- pchisq(manymods_ifit[[i]]$i.fit, df=manymods_ifit[[i]]$i.df-1, lower.tail=FALSE)
  x <- t(as.data.frame(ifelse(p<=.05,TRUE, FALSE)))
  items <- data.frame(model=trolls_models$group_models[[i]]$group,
                      x,
                      stringsAsFactors = FALSE,
                      row.names = trolls_models$group_models[[i]]$group)
  z<-c("model","AFRAID", "AMUSED", "ANGRY", "ANNOYED", "DONT_CARE", "HAPPY",
       "INSPIRED", "SAD")
  if(ncol(items) < 9){
    items[,z[which(!z%in% colnames(items))]]<-NA
  }
  items
}, simplify = F)


manymod_ifit_total<-do.call(rbind, manymods_ifit_total)

manymod_ifit_tab <- as.data.frame(rbind(
  paste0(round(colSums(manymod_ifit_total[modsL,-1], 
              na.rm = T)/nrow(manymod_ifit_total[modsL,]),2)*100,"%"),
  paste0(round(colSums(manymod_ifit_total[!modsL,-1], 
              na.rm = T)/nrow(manymod_ifit_total[!modsL,]),2)*100,"%")
))
colnames(manymod_ifit_tab) <- c("Afraid", "Amused", "Angry", "Annoyed",
                                "Don't Care", "Happy", "Inspired", "Sad")

manymod_ifit_tab$Group <- c("Left-wing", "Right-wing")

manymod_ifit_tab<-manymod_ifit_tab[,c( "Group","Afraid", "Amused", "Angry",
                                       "Annoyed","Don't Care", "Happy",
                                       "Inspired", "Sad")]


```

## Item Category Ordering

```{r categories}
# Item category ordering -----

# create vectors of left and right trolls
catleft <- row.names(trolls_models$category_order) %in%
  author.small[which(author.small$account_type == "Left"), "author"]
catright <- row.names(trolls_models$category_order) %in%
  author.small[which(author.small$account_type == "Right"), "author"]

# check
x<-row.names(trolls_models$category_order)[catleft]
sum(!x %in% trolls$author[which(trolls$account_type == "Left")])

x<-row.names(trolls_models$category_order)[catright]
sum(!x %in% trolls$author[which(trolls$account_type == "Right")])

table(author.small$account_type)["Left"] == sum(catleft)
table(author.small$account_type)["Right"] == sum(catright)


# category plots


cbbPalette <- c("#E69F00", "#999999", "#56B4E9", 
                "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# catplots + ggtitle("Category Order Plot - All Trolls")

lab1 <- c(`AMUSED` = "Amused", `ANGRY` = "Angry", `ANNOYED` = "Annoyed", `DONT_CARE` = "Don't Care", `INSPIRED` = "Inspired", `SAD` = "Sad", `AFRAID` = "Afraid", `HAPPY` = "Happy")

dat_text_R <- data.frame(
  # label = c("Amused", "Angry", "Annoyed", "Don't Care", 
  #           "Inspired", "Sad", "Afraid", "Happy"),
  label = c(" ", "\u002A \u2020", "\u002A \u2020", " \u2020", " ", "\u002A \u2020", "\u002A \u2020", " "),
  Category   = colnames(trolls_models$category_order),
  x     = c(5),
  y     = c(400)
)

dat_text_L <- data.frame(
  # label = c("Amused", "Angry", "Annoyed", "Don't Care", 
  #           "Inspired", "Sad", "Afraid", "Happy"),
  label = c(" ", "\u002A \u2020", "\u002A \u2020", " \u2020", " ", "\u002A \u2020", "\u002A \u2020", " "),
  Category   = colnames(trolls_models$category_order),
  x     = c(5),
  y     = c(150)
)

catplotsL<-  catplot(trolls_models$category_order[catleft,],
                     cat_labels = lab1) +
  ggtitle("Category Order Plot - Left-wing Trolls") + 
  theme_apa() + scale_fill_manual(values=cbbPalette) +
  scale_colour_manual(values=cbbPalette) + theme(legend.position = "none")   +
                 geom_text(
                 data    = dat_text_L,
                 mapping = aes(x = x, y = y, label = label), color = 'black'
               )

catplotsL

catplotsR<- catplot(trolls_models$category_order[catright,],
                    cat_labels = lab1) + 
  ggtitle("Category Order Plot - Right-wing Trolls") + 
  theme_apa() + scale_fill_manual(values=cbbPalette) +
  scale_colour_manual(values=cbbPalette) + theme(legend.position = "none") +
                geom_text(
                 data    = dat_text_R,
                 mapping = aes(x = x, y = y, label = label), color = 'black'
               )

catplotsR

describe(trolls_models$category_order)
describe(trolls_models$category_order[catright,])
describe(trolls_models$category_order[catleft,])
```

Now I do two tests to see if these distributions are different, t-test (mean) and a Kolmogorov-Smirnov Tests (distribution). 

```{r tests}
dat<- data.frame(
  cbind(trolls_models$category_order,
        group=catleft))

aggregate(. ~ group,dat, FUN=mean) # false = right

# t tests
ttests<-lapply(colnames(trolls_models$category_order), function(i){
  t.test(trolls_models$category_order[catright,i],
         trolls_models$category_order[catleft,i])
})
names(ttests) <- colnames(trolls_models$category_order)
# pvalues
cat("\n pvalues for t test \n")
sapply(1:length(colnames(trolls_models$category_order)), function(num){
  rbind(colnames(trolls_models$category_order)[num],
        round(ttests[[num]]$p.value,3))
})


# distribution tests
kstests<-suppressWarnings(lapply(colnames(trolls_models$category_order), function(i){
  ks.test(trolls_models$category_order[catright,i],
          trolls_models$category_order[catleft,i])
}))
names(kstests) <- colnames(trolls_models$category_order)
# pvalues
cat("\n pvalues for KS test \n")
sapply(1:length(colnames(trolls_models$category_order)), function(num){
  rbind(colnames(trolls_models$category_order)[num],
        round(kstests[[num]]$p.value,3))
})


```


We can do the same for the standard deviations using levene's test

```{r}
lvtestsd<-sapply(1:8, function(x){car::leveneTest( trolls_models$category_order[,x] ~ as.factor(catright))},
                 simplify = F)

names(lvtestsd) <- colnames(trolls_models$category_order)

which(lvtestsd[3,] < .05)

sapply(1:length(colnames(trolls_models$category_order)), function(num){
  rbind(colnames(trolls_models$category_order)[num],
        round(lvtestsd[[num]]$`Pr(>F)`,3))
})

```

## Mutual Information

First, I ran a quick simulation to determine the minimum amount of mutual information (where higher values = less information). Then, I calculated MI for left and right trolls, and then bootstrapped confidence intervals. Looking at the information from the bootstrapping, the MI is slightly positively biased (approx. +.30).



```{r mutual info}
# Mutual Information -----

# Random MI

n<-length(trolls_models$group_models)
ranMi <- matrix(nrow=n, ncol=8)
for(i in 1:n){
  ranMi[i,] <- sample(1:8, 8,replace = FALSE)}

boot.random <- boot(ranMi,
                    statistic = mi, R=1000)
bci.random<-boot.ci(boot.random, conf = .95, type = "norm")

# Calculate mutual information per group

multiinformation(trolls_models$category_order)
multiinformation(trolls_models$category_order[catright,])
multiinformation(trolls_models$category_order[catleft,])

# bootstrap CI for mutual information
mi <- function(data, indices) {
  d <- data[indices,] # allows boot to select sample
  mi<- multiinformation(data[indices,])
  return((mi))
}

boot.left <-boot(trolls_models$category_order
                 [which(catleft),],
                 statistic = mi, R=1000)
boot.left
bci.left<-boot.ci(boot.left, conf = .95, type = "norm")


boot.right <-boot(trolls_models$category_order
                  [which(catright),],
                  statistic = mi, R=1000)
boot.right
bci.right<-boot.ci(boot.right, conf = .95, type = "norm")



# plot

plotdat2 <- data.frame(
  MI = c(multiinformation(trolls_models$category_order[which(catright),]),
             multiinformation(trolls_models$category_order[which(catleft),])),
  ci.low =  c(bci.right$normal[2],
             bci.left$normal[2]),
  ci.high =  c(bci.right$normal[3],
             bci.left$normal[3]),
  Group = c("Right", "Left")
)
plotdat2
ggplot(data = plotdat2) + geom_point(aes(x=Group, y=MI), color = c("Red", "Blue")) +
  geom_hline(yintercept = boot.random$t0, linetype="dashed") +
  geom_errorbar(aes(ymin=ci.low, ymax=ci.high, x = Group), colour="black", width=.1) +
  ggtitle("Mutual Information by Group w/ 95% CI") + ylim(0,12) +
  # annotate("text", label="Maximum value (minimum info)",
  #          x=1, y=min_info+.3,
  #          size=4, fontface="italic") +
  ylab("Mutual Information")

```


## Emodiversity
Quick look at the "emodiversity" of the trolls (Benson, 2017). This is done using the gini coefficient. Note that the coefficient fails (is NA) when the whole vector is zeros.

I calculated it per tweet, and within authors (suming up emotion scores, and then calculating it), and average tweets per author (calculating it per tweet, then averaging within an author, all NA's removed). For each grouping, I calculated it with the raw emotion scores and the dichotomized emotion scores.

```{r diversity}
# Diversity -----
# Gini Coefficient
# FYI, it fails when all responses are zero, row gets NA


# Gini by tweet
gini_trolls_tweets <- data.frame(full = apply(trolls_scored[,c(-1,-2)],1,reldist::gini),
                                 dich = apply(trolls_models$full_model$X,1,reldist::gini),
                                 group = trolls$account_type)
# Gini by author
gini_trolls_author <-
  data.frame(
    full = sapply(unique(trolls$author), function(group) {
      x <- trolls_scored[trolls$author == group, ]
      if(ncol(x) != 10){NA}else{
        reldist::gini(colSums(x[, c("AFRAID",
                                    "AMUSED",
                                    "ANGRY",
                                    "ANNOYED",
                                    "DONT_CARE",
                                    "HAPPY",
                                    "INSPIRED",
                                    "SAD")]))}
    }),

    dich = sapply(unique(trolls$author), function(group) {
      x <- trolls_models$full_model$X[trolls$author == group, ]
      if(length(x) <= 8){z<-NA}else{
        reldist::gini(colSums(x[, c("AFRAID",
                                    "AMUSED",
                                    "ANGRY",
                                    "ANNOYED",
                                    "DONT_CARE",
                                    "HAPPY",
                                    "INSPIRED",
                                    "SAD")]))}
    }),
    group = unique(trolls[trolls$author %in% unique(trolls$author),c("author","account_type")])$account_type,
    author = unique(trolls$author)
  )

# Average gini by tweet within authors
gini_trolls_tweets_average <- do.call(rbind, lapply(unique(trolls$author), function(auth){
  x <- gini_trolls_tweets[trolls$author == auth, ]
  averages <- data.frame(
    full = mean(na.omit(x$full)),
    dich = mean(na.omit(x$dich)),
    author = as.character(auth),
    group = x$group[1],
    stringsAsFactors = FALSE
  )
  averages
}))


```



```{r diversitytests}

t.test(full ~ group, gini_trolls_tweets)
t.test(full ~ group, gini_trolls_author)
t.test(full ~ group, gini_trolls_tweets_average)

ginitweetresults<-t.test(full ~ group, gini_trolls_tweets)
giniauthorresults<-t.test(full ~ group, gini_trolls_author)
```

## Notes

* All Right/Left troll tweets are included in the full Rasch model. Only tweets belonging to accounts with > 29 tweets are included in the individual model accounts. 
* I removed manual retweets/reposts. Should I consider leaving them in? On one hand, we do not know who "wrote" them. They also may be duplicated between themselves - that is, they are retweeting each other. On the other hand, they are a part of the accounts' overall "voices.""

## 

```{r descript}
tweets30<-sapply(author.small$author,function(z){
  nrow(trolls[which(trolls$author==z),])
})
tweets30right <- sum(tweets30[which(author.small$account_type=="Right")])
tweets30left <- sum(tweets30[which(author.small$account_type=="Left")])

tweetsbyaccount <- table(trolls$author)
median(tweetsbyaccount)

```

```{r dimensions}
library(pairwise)
# Obtain person parameters using the "pairwise" package functions
pers <- pers(pair(trolls_models$full_model$X))
# Perform the PCA
rasch_pca <- rfa(pers)
```

```{r}

x<-which(sapply(trolls_models$group_models, function(x) !is.null(x$warnings)))

z<- vector("list", length(x))
for(i in 1:length(x)){
  z[[i]]<-(trolls_models$group_models[[x[i]]]$warnings)
}

err_models<-sapply(z, function(x){ stringr::str_extract_all(x, gsub(", ", "|", toString(c("AFRAID", "AMUSED", "ANGRY", "ANNOYED", "DONT_CARE", 
"HAPPY", "INSPIRED", "SAD"))),
                                 simplify = FALSE)})
err_models<-do.call(rbind, err_models)

```
### Prediction

```{r}
library(lme4)
lmdat <- cbind(trolls, trolls_scored)
lmdat$rawscore <- rowSums(trolls_models$full_model$X)
lmdat$followers_scale <- scale(lmdat$followers)
lmdat$updates_scale <- scale(lmdat$updates)

emo_mod1 <- lmer(updates_scale ~ followers_scale + rawscore + 
                   # AFRAID + AMUSED + ANGRY + ANNOYED + 
                   # DONT_CARE + HAPPY + INSPIRED + SAD +
                   (1|account_type),
               data = lmdat)
summary(emo_mod1)

```
